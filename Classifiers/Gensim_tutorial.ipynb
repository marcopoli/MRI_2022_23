{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsxP6VdezGId",
        "outputId": "971bf707-0a9d-4501-d5c3-c10d24e63e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kAaBewyzGIm"
      },
      "source": [
        "Create a Dictionary from a list of sentences\n",
        "======================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLcngJEzGIo"
      },
      "source": [
        "In gensim, the dictionary contains a map of all words (tokens) to its unique id.\n",
        "\n",
        "You can create a dictionary from a paragraph of sentences, from a text file that contains multiple lines of text and from multiple such text files contained in a directory. For the second and third cases, we will do it without loading the entire file into memory so that the dictionary gets updated as you read the text line by line.\n",
        "\n",
        "Let’s start with the ‘List of sentences’ input.\n",
        "\n",
        "When you have multiple sentences, you need to convert each sentence to a list of words. List comprehensions is a common way to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4MNp2PJzGIp",
        "outputId": "ce275cf0-a37b-4730-d77d-0b4bc6e42706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from pprint import pprint\n",
        "\n",
        "# How to create a dictionary from a list of sentences?\n",
        "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
        "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
        "             \"interrogation that went wrong, one that was intended to lead\", \n",
        "             \"to his abduction from Turkey, according to two sources.\"]\n",
        "\n",
        "documents_2 = [\"One source says the report will likely conclude that\", \n",
        "                \"the operation was carried out without clearance and\", \n",
        "                \"transparency and that those involved will be held\", \n",
        "                \"responsible. One of the sources acknowledged that the\", \n",
        "                \"report is still being prepared and cautioned that\", \n",
        "                \"things could change.\"]\n",
        "\n",
        "# Tokenize(split) the sentences into words\n",
        "texts = [[text for text in doc.split()] for doc in documents]\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Get information about the dictionary\n",
        "print(dictionary)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXo52UvdzGIv"
      },
      "source": [
        "As it says the dictionary has 33 unique tokens (or words). Let’s see the unique ids for each of these tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNWf4tnfzGIw",
        "outputId": "adf92f04-f228-4be7-97c9-f7fca813ca19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show the word to id map\n",
        "print(dictionary.token2id)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO8QC5DWzGI2"
      },
      "source": [
        "We have successfully created a Dictionary object. Gensim will use this dictionary to create a bag-of-words corpus where the words in the documents are replaced with its respective id provided by this dictionary.\n",
        "\n",
        "If you get new documents in the future, it is also possible to update an existing dictionary to include the new words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8tVegRYzGI4",
        "outputId": "d6790410-80fa-4bd8-e728-3107a71befce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "documents_2 = [\"The intersection graph of paths in trees\",\n",
        "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "               \"Graph minors A survey\"]\n",
        "\n",
        "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
        "\n",
        "dictionary.add_documents(texts_2)\n",
        "\n",
        "\n",
        "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
        "print(dictionary)\n",
        "\n",
        "print(dictionary.token2id)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
            "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yKvje1vzGI9"
      },
      "source": [
        "Create a Dictionary from one file\n",
        "============================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjhYoo7OzGI-"
      },
      "source": [
        "You can also create a dictionary from a text file.\n",
        "\n",
        "The below example reads a file line-by-line and uses gensim’s simple_preprocess to process one line of the file at a time.\n",
        "\n",
        "The advantage here is it let’s you read an entire text file without loading the file in memory all at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OMXgfCdzGI_",
        "outputId": "0baf2799-02ac-40d6-d8bc-8ab368ce346e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from smart_open import smart_open\n",
        "import os\n",
        "\n",
        "# Create gensim dictionary form a single tet file, deacc=True -> remove accent marks from tokens\n",
        "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('Alice_lines_utf8.txt', encoding='utf-8'))\n",
        "\n",
        "# Token to Id map\n",
        "dictionary.token2id\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adventures': 0,\n",
              " 'alice': 1,\n",
              " 'in': 2,\n",
              " 'wonderland': 3,\n",
              " 'carroll': 4,\n",
              " 'lewis': 5,\n",
              " 'edition': 6,\n",
              " 'fulcrum': 7,\n",
              " 'millennium': 8,\n",
              " 'the': 9,\n",
              " 'chapter': 10,\n",
              " 'down': 11,\n",
              " 'hole': 12,\n",
              " 'rabbit': 13,\n",
              " 'and': 14,\n",
              " 'bank': 15,\n",
              " 'beginning': 16,\n",
              " 'book': 17,\n",
              " 'but': 18,\n",
              " 'by': 19,\n",
              " 'conversations': 20,\n",
              " 'do': 21,\n",
              " 'get': 22,\n",
              " 'had': 23,\n",
              " 'having': 24,\n",
              " 'her': 25,\n",
              " 'into': 26,\n",
              " 'is': 27,\n",
              " 'it': 28,\n",
              " 'no': 29,\n",
              " 'nothing': 30,\n",
              " 'of': 31,\n",
              " 'on': 32,\n",
              " 'once': 33,\n",
              " 'or': 34,\n",
              " 'peeped': 35,\n",
              " 'pictures': 36,\n",
              " 'reading': 37,\n",
              " 'she': 38,\n",
              " 'sister': 39,\n",
              " 'sitting': 40,\n",
              " 'thought': 41,\n",
              " 'tired': 42,\n",
              " 'to': 43,\n",
              " 'twice': 44,\n",
              " 'use': 45,\n",
              " 'very': 46,\n",
              " 'was': 47,\n",
              " 'what': 48,\n",
              " 'without': 49,\n",
              " 'as': 50,\n",
              " 'be': 51,\n",
              " 'chain': 52,\n",
              " 'close': 53,\n",
              " 'considering': 54,\n",
              " 'could': 55,\n",
              " 'daisies': 56,\n",
              " 'daisy': 57,\n",
              " 'day': 58,\n",
              " 'eyes': 59,\n",
              " 'feel': 60,\n",
              " 'for': 61,\n",
              " 'getting': 62,\n",
              " 'hot': 63,\n",
              " 'made': 64,\n",
              " 'making': 65,\n",
              " 'mind': 66,\n",
              " 'own': 67,\n",
              " 'picking': 68,\n",
              " 'pink': 69,\n",
              " 'pleasure': 70,\n",
              " 'ran': 71,\n",
              " 'sleepy': 72,\n",
              " 'so': 73,\n",
              " 'stupid': 74,\n",
              " 'suddenly': 75,\n",
              " 'trouble': 76,\n",
              " 'up': 77,\n",
              " 'well': 78,\n",
              " 'when': 79,\n",
              " 'whether': 80,\n",
              " 'white': 81,\n",
              " 'with': 82,\n",
              " 'worth': 83,\n",
              " 'would': 84,\n",
              " 'across': 85,\n",
              " 'actually': 86,\n",
              " 'after': 87,\n",
              " 'afterwards': 88,\n",
              " 'all': 89,\n",
              " 'at': 90,\n",
              " 'before': 91,\n",
              " 'burning': 92,\n",
              " 'curiosity': 93,\n",
              " 'dear': 94,\n",
              " 'did': 95,\n",
              " 'either': 96,\n",
              " 'feet': 97,\n",
              " 'field': 98,\n",
              " 'flashed': 99,\n",
              " 'fortunately': 100,\n",
              " 'have': 101,\n",
              " 'hear': 102,\n",
              " 'hedge': 103,\n",
              " 'hurried': 104,\n",
              " 'its': 105,\n",
              " 'itself': 106,\n",
              " 'just': 107,\n",
              " 'large': 108,\n",
              " 'late': 109,\n",
              " 'looked': 110,\n",
              " 'much': 111,\n",
              " 'natural': 112,\n",
              " 'never': 113,\n",
              " 'nor': 114,\n",
              " 'occurred': 115,\n",
              " 'oh': 116,\n",
              " 'ought': 117,\n",
              " 'out': 118,\n",
              " 'over': 119,\n",
              " 'pocket': 120,\n",
              " 'pop': 121,\n",
              " 'quite': 122,\n",
              " 'remarkable': 123,\n",
              " 'say': 124,\n",
              " 'see': 125,\n",
              " 'seemed': 126,\n",
              " 'seen': 127,\n",
              " 'shall': 128,\n",
              " 'started': 129,\n",
              " 'take': 130,\n",
              " 'that': 131,\n",
              " 'then': 132,\n",
              " 'there': 133,\n",
              " 'think': 134,\n",
              " 'this': 135,\n",
              " 'time': 136,\n",
              " 'took': 137,\n",
              " 'under': 138,\n",
              " 'waistcoat': 139,\n",
              " 'watch': 140,\n",
              " 'way': 141,\n",
              " 'wondered': 142,\n",
              " 'again': 143,\n",
              " 'another': 144,\n",
              " 'how': 145,\n",
              " 'moment': 146,\n",
              " 'went': 147,\n",
              " 'world': 148,\n",
              " 'about': 149,\n",
              " 'deep': 150,\n",
              " 'dipped': 151,\n",
              " 'falling': 152,\n",
              " 'found': 153,\n",
              " 'herself': 154,\n",
              " 'like': 155,\n",
              " 'not': 156,\n",
              " 'some': 157,\n",
              " 'stopping': 158,\n",
              " 'straight': 159,\n",
              " 'tunnel': 160,\n",
              " 'anything': 161,\n",
              " 'coming': 162,\n",
              " 'cupboards': 163,\n",
              " 'dark': 164,\n",
              " 'disappointment': 165,\n",
              " 'drop': 166,\n",
              " 'empty': 167,\n",
              " 'fear': 168,\n",
              " 'fell': 169,\n",
              " 'filled': 170,\n",
              " 'first': 171,\n",
              " 'from': 172,\n",
              " 'going': 173,\n",
              " 'great': 174,\n",
              " 'happen': 175,\n",
              " 'here': 176,\n",
              " 'hung': 177,\n",
              " 'jar': 178,\n",
              " 'killing': 179,\n",
              " 'labelled': 180,\n",
              " 'look': 181,\n",
              " 'make': 182,\n",
              " 'managed': 183,\n",
              " 'maps': 184,\n",
              " 'marmalade': 185,\n",
              " 'next': 186,\n",
              " 'noticed': 187,\n",
              " 'one': 188,\n",
              " 'orange': 189,\n",
              " 'passed': 190,\n",
              " 'past': 191,\n",
              " 'pegs': 192,\n",
              " 'plenty': 193,\n",
              " 'put': 194,\n",
              " 'saw': 195,\n",
              " 'shelves': 196,\n",
              " 'sides': 197,\n",
              " 'slowly': 198,\n",
              " 'somebody': 199,\n",
              " 'they': 200,\n",
              " 'too': 201,\n",
              " 'tried': 202,\n",
              " 'upon': 203,\n",
              " 'were': 204,\n",
              " 'wonder': 205,\n",
              " 'brave': 206,\n",
              " 'even': 207,\n",
              " 'fall': 208,\n",
              " 'home': 209,\n",
              " 'house': 210,\n",
              " 'if': 211,\n",
              " 'likely': 212,\n",
              " 'll': 213,\n",
              " 'me': 214,\n",
              " 'off': 215,\n",
              " 'stairs': 216,\n",
              " 'such': 217,\n",
              " 'top': 218,\n",
              " 'true': 219,\n",
              " 'tumbling': 220,\n",
              " 'which': 221,\n",
              " 'why': 222,\n",
              " 'wouldn': 223,\n",
              " 'aloud': 224,\n",
              " 'an': 225,\n",
              " 'centre': 226,\n",
              " 'come': 227,\n",
              " 'distance': 228,\n",
              " 'earth': 229,\n",
              " 'end': 230,\n",
              " 'fallen': 231,\n",
              " 'four': 232,\n",
              " 'good': 233,\n",
              " 'got': 234,\n",
              " 'grand': 235,\n",
              " 'idea': 236,\n",
              " 'knowledge': 237,\n",
              " 'latitude': 238,\n",
              " 'learnt': 239,\n",
              " 'lessons': 240,\n",
              " 'let': 241,\n",
              " 'listen': 242,\n",
              " 'longitude': 243,\n",
              " 'many': 244,\n",
              " 'miles': 245,\n",
              " 'must': 246,\n",
              " 'near': 247,\n",
              " 'nice': 248,\n",
              " 'opportunity': 249,\n",
              " 'practice': 250,\n",
              " 'right': 251,\n",
              " 'said': 252,\n",
              " 'schoolroom': 253,\n",
              " 'several': 254,\n",
              " 'showing': 255,\n",
              " 'somewhere': 256,\n",
              " 'sort': 257,\n",
              " 'still': 258,\n",
              " 'things': 259,\n",
              " 'though': 260,\n",
              " 'thousand': 261,\n",
              " 've': 262,\n",
              " 'words': 263,\n",
              " 'yes': 264,\n",
              " 'you': 265,\n",
              " 'air': 266,\n",
              " 'am': 267,\n",
              " 'among': 268,\n",
              " 'antipathies': 269,\n",
              " 'ask': 270,\n",
              " 'asking': 271,\n",
              " 'australia': 272,\n",
              " 'began': 273,\n",
              " 'country': 274,\n",
              " 'curtsey': 275,\n",
              " 'curtseying': 276,\n",
              " 'didn': 277,\n",
              " 'downward': 278,\n",
              " 'fancy': 279,\n",
              " 'funny': 280,\n",
              " 'girl': 281,\n",
              " 'glad': 282,\n",
              " 'heads': 283,\n",
              " 'ignorant': 284,\n",
              " 'know': 285,\n",
              " 'listening': 286,\n",
              " 'little': 287,\n",
              " 'ma': 288,\n",
              " 'manage': 289,\n",
              " 'name': 290,\n",
              " 'new': 291,\n",
              " 'people': 292,\n",
              " 'perhaps': 293,\n",
              " 'please': 294,\n",
              " 'presently': 295,\n",
              " 'rather': 296,\n",
              " 're': 297,\n",
              " 'seem': 298,\n",
              " 'sound': 299,\n",
              " 'spoke': 300,\n",
              " 'their': 301,\n",
              " 'them': 302,\n",
              " 'through': 303,\n",
              " 'walk': 304,\n",
              " 'word': 305,\n",
              " 'written': 306,\n",
              " 'zealand': 307,\n",
              " 'afraid': 308,\n",
              " 'answer': 309,\n",
              " 'are': 310,\n",
              " 'bat': 311,\n",
              " 'bats': 312,\n",
              " 'begun': 313,\n",
              " 'came': 314,\n",
              " 'cat': 315,\n",
              " 'catch': 316,\n",
              " 'cats': 317,\n",
              " 'couldn': 318,\n",
              " 'dinah': 319,\n",
              " 'dozing': 320,\n",
              " 'dream': 321,\n",
              " 'dreamy': 322,\n",
              " 'dry': 323,\n",
              " 'earnestly': 324,\n",
              " 'eat': 325,\n",
              " 'else': 326,\n",
              " 'ever': 327,\n",
              " 'felt': 328,\n",
              " 'hand': 329,\n",
              " 'heap': 330,\n",
              " 'hope': 331,\n",
              " 'leaves': 332,\n",
              " 'matter': 333,\n",
              " 'mice': 334,\n",
              " 'might': 335,\n",
              " 'milk': 336,\n",
              " 'miss': 337,\n",
              " 'mouse': 338,\n",
              " 'my': 339,\n",
              " 'night': 340,\n",
              " 'now': 341,\n",
              " 'question': 342,\n",
              " 'remember': 343,\n",
              " 'saucer': 344,\n",
              " 'saying': 345,\n",
              " 'should': 346,\n",
              " 'sometimes': 347,\n",
              " 'soon': 348,\n",
              " 'sticks': 349,\n",
              " 'talking': 350,\n",
              " 'tea': 351,\n",
              " 'tell': 352,\n",
              " 'thump': 353,\n",
              " 'truth': 354,\n",
              " 'walking': 355,\n",
              " 'wish': 356,\n",
              " 'away': 357,\n",
              " 'behind': 358,\n",
              " 'bit': 359,\n",
              " 'corner': 360,\n",
              " 'ears': 361,\n",
              " 'hall': 362,\n",
              " 'hanging': 363,\n",
              " 'hurrying': 364,\n",
              " 'hurt': 365,\n",
              " 'jumped': 366,\n",
              " 'lamps': 367,\n",
              " 'lit': 368,\n",
              " 'long': 369,\n",
              " 'longer': 370,\n",
              " 'lost': 371,\n",
              " 'low': 372,\n",
              " 'overhead': 373,\n",
              " 'passage': 374,\n",
              " 'roof': 375,\n",
              " 'row': 376,\n",
              " 'sight': 377,\n",
              " 'turned': 378,\n",
              " 'whiskers': 379,\n",
              " 'wind': 380,\n",
              " 'been': 381,\n",
              " 'door': 382,\n",
              " 'doors': 383,\n",
              " 'every': 384,\n",
              " 'locked': 385,\n",
              " 'middle': 386,\n",
              " 'other': 387,\n",
              " 'round': 388,\n",
              " 'sadly': 389,\n",
              " 'side': 390,\n",
              " 'trying': 391,\n",
              " 'walked': 392,\n",
              " 'wondering': 393,\n",
              " 'alas': 394,\n",
              " 'any': 395,\n",
              " 'belong': 396,\n",
              " 'curtain': 397,\n",
              " 'delight': 398,\n",
              " 'except': 399,\n",
              " 'fifteen': 400,\n",
              " 'fitted': 401,\n",
              " 'glass': 402,\n",
              " 'golden': 403,\n",
              " 'high': 404,\n",
              " 'however': 405,\n",
              " 'inches': 406,\n",
              " 'key': 407,\n",
              " 'legged': 408,\n",
              " 'lock': 409,\n",
              " 'locks': 410,\n",
              " 'open': 411,\n",
              " 'rate': 412,\n",
              " 'second': 413,\n",
              " 'small': 414,\n",
              " 'solid': 415,\n",
              " 'table': 416,\n",
              " 'three': 417,\n",
              " 'tiny': 418,\n",
              " 'along': 419,\n",
              " 'beds': 420,\n",
              " 'begin': 421,\n",
              " 'bright': 422,\n",
              " 'cool': 423,\n",
              " 'doorway': 424,\n",
              " 'few': 425,\n",
              " 'flowers': 426,\n",
              " 'fountains': 427,\n",
              " 'garden': 428,\n",
              " 'go': 429,\n",
              " 'happened': 430,\n",
              " 'head': 431,\n",
              " 'impossible': 432,\n",
              " 'indeed': 433,\n",
              " 'knelt': 434,\n",
              " 'knew': 435,\n",
              " 'larger': 436,\n",
              " 'lately': 437,\n",
              " 'led': 438,\n",
              " 'longed': 439,\n",
              " 'loveliest': 440,\n",
              " 'only': 441,\n",
              " 'opened': 442,\n",
              " 'poor': 443,\n",
              " 'rat': 444,\n",
              " 'really': 445,\n",
              " 'shoulders': 446,\n",
              " 'shut': 447,\n",
              " 'telescope': 448,\n",
              " 'than': 449,\n",
              " 'those': 450,\n",
              " 'wander': 451,\n",
              " 'back': 452,\n",
              " 'beautifully': 453,\n",
              " 'bottle': 454,\n",
              " 'certainly': 455,\n",
              " 'drink': 456,\n",
              " 'find': 457,\n",
              " 'half': 458,\n",
              " 'hoping': 459,\n",
              " 'label': 460,\n",
              " 'letters': 461,\n",
              " 'neck': 462,\n",
              " 'paper': 463,\n",
              " 'printed': 464,\n",
              " 'rules': 465,\n",
              " 'shutting': 466,\n",
              " 'telescopes': 467,\n",
              " 'waiting': 468,\n",
              " 'almost': 469,\n",
              " 'beasts': 470,\n",
              " 'because': 471,\n",
              " 'bleeds': 472,\n",
              " 'burn': 473,\n",
              " 'burnt': 474,\n",
              " 'certain': 475,\n",
              " 'children': 476,\n",
              " 'cut': 477,\n",
              " 'deeply': 478,\n",
              " 'disagree': 479,\n",
              " 'eaten': 480,\n",
              " 'finger': 481,\n",
              " 'forgotten': 482,\n",
              " 'friends': 483,\n",
              " 'histories': 484,\n",
              " 'hold': 485,\n",
              " 'hurry': 486,\n",
              " 'knife': 487,\n",
              " 'later': 488,\n",
              " 'marked': 489,\n",
              " 'poison': 490,\n",
              " 'poker': 491,\n",
              " 'read': 492,\n",
              " 'red': 493,\n",
              " 'simple': 494,\n",
              " 'sooner': 495,\n",
              " 'taught': 496,\n",
              " 'unpleasant': 497,\n",
              " 'usually': 498,\n",
              " 'who': 499,\n",
              " 'wild': 500,\n",
              " 'will': 501,\n",
              " 'wise': 502,\n",
              " 'your': 503,\n",
              " 'apple': 504,\n",
              " 'buttered': 505,\n",
              " 'cherry': 506,\n",
              " 'custard': 507,\n",
              " 'fact': 508,\n",
              " 'finding': 509,\n",
              " 'finished': 510,\n",
              " 'flavour': 511,\n",
              " 'mixed': 512,\n",
              " 'pine': 513,\n",
              " 'roast': 514,\n",
              " 'tart': 515,\n",
              " 'taste': 516,\n",
              " 'toast': 517,\n",
              " 'toffee': 518,\n",
              " 'turkey': 519,\n",
              " 'ventured': 520,\n",
              " 'curious': 521,\n",
              " 'feeling': 522,\n",
              " 'altogether': 523,\n",
              " 'blown': 524,\n",
              " 'brightened': 525,\n",
              " 'candle': 526,\n",
              " 'face': 527,\n",
              " 'flame': 528,\n",
              " 'further': 529,\n",
              " 'lovely': 530,\n",
              " 'minutes': 531,\n",
              " 'nervous': 532,\n",
              " 'shrink': 533,\n",
              " 'size': 534,\n",
              " 'ten': 535,\n",
              " 'thing': 536,\n",
              " 'waited': 537,\n",
              " 'best': 538,\n",
              " 'climb': 539,\n",
              " 'cried': 540,\n",
              " 'decided': 541,\n",
              " 'legs': 542,\n",
              " 'more': 543,\n",
              " 'plainly': 544,\n",
              " 'possibly': 545,\n",
              " 'reach': 546,\n",
              " 'sat': 547,\n",
              " 'slippery': 548,\n",
              " 'while': 549,\n",
              " 'advice': 550,\n",
              " 'advise': 551,\n",
              " 'against': 552,\n",
              " 'box': 553,\n",
              " 'bring': 554,\n",
              " 'cheated': 555,\n",
              " 'child': 556,\n",
              " 'croquet': 557,\n",
              " 'crying': 558,\n",
              " 'enough': 559,\n",
              " 'followed': 560,\n",
              " 'fond': 561,\n",
              " 'game': 562,\n",
              " 'gave': 563,\n",
              " 'generally': 564,\n",
              " 'hardly': 565,\n",
              " 'leave': 566,\n",
              " 'left': 567,\n",
              " 'minute': 568,\n",
              " 'person': 569,\n",
              " 'playing': 570,\n",
              " 'pretend': 571,\n",
              " 'pretending': 572,\n",
              " 'remembered': 573,\n",
              " 'respectable': 574,\n",
              " 'scolded': 575,\n",
              " 'seldom': 576,\n",
              " 'severely': 577,\n",
              " 'sharply': 578,\n",
              " 'tears': 579,\n",
              " 'two': 580,\n",
              " 'cake': 581,\n",
              " 'can': 582,\n",
              " 'care': 583,\n",
              " 'creep': 584,\n",
              " 'currants': 585,\n",
              " 'don': 586,\n",
              " 'eye': 587,\n",
              " 'grow': 588,\n",
              " 'happens': 589,\n",
              " 'lying': 590,\n",
              " 'makes': 591,\n",
              " 'smaller': 592,\n",
              " 'anxiously': 593,\n",
              " 'ate': 594,\n",
              " 'common': 595,\n",
              " 'dull': 596,\n",
              " 'eats': 597,\n",
              " 'expecting': 598,\n",
              " 'growing': 599,\n",
              " 'holding': 600,\n",
              " 'life': 601,\n",
              " 'remained': 602,\n",
              " 'same': 603,\n",
              " 'sure': 604,\n",
              " 'surprised': 605,\n",
              " 'set': 606,\n",
              " 'work': 607,\n",
              " 'ii': 608,\n",
              " 'pool': 609,\n",
              " 'able': 610,\n",
              " 'boots': 611,\n",
              " 'bye': 612,\n",
              " 'christmas': 613,\n",
              " 'curiouser': 614,\n",
              " 'deal': 615,\n",
              " 'dears': 616,\n",
              " 'english': 617,\n",
              " 'far': 618,\n",
              " 'forgot': 619,\n",
              " 'give': 620,\n",
              " 'kind': 621,\n",
              " 'largest': 622,\n",
              " 'myself': 623,\n",
              " 'opening': 624,\n",
              " 'pair': 625,\n",
              " 'shan': 626,\n",
              " 'shoes': 627,\n",
              " 'speak': 628,\n",
              " 'stockings': 629,\n",
              " 'want': 630,\n",
              " 'won': 631,\n",
              " 'carrier': 632,\n",
              " 'directions': 633,\n",
              " 'odd': 634,\n",
              " 'planning': 635,\n",
              " 'presents': 636,\n",
              " 'sending': 637,\n",
              " 'esq': 638,\n",
              " 'fender': 639,\n",
              " 'foot': 640,\n",
              " 'hearthrug': 641,\n",
              " 'love': 642,\n",
              " 'nonsense': 643,\n",
              " 'nine': 644,\n",
              " 'struck': 645,\n",
              " 'cry': 646,\n",
              " 'hopeless': 647,\n",
              " 'ashamed': 648,\n",
              " 'gallons': 649,\n",
              " 'reaching': 650,\n",
              " 'shedding': 651,\n",
              " 'stop': 652,\n",
              " 'until': 653,\n",
              " 'yourself': 654,\n",
              " 'darkness': 655,\n",
              " 'desperate': 656,\n",
              " 'dressed': 657,\n",
              " 'dried': 658,\n",
              " 'dropped': 659,\n",
              " 'duchess': 660,\n",
              " 'fan': 661,\n",
              " 'gloves': 662,\n",
              " 'hard': 663,\n",
              " 'hastily': 664,\n",
              " 'he': 665,\n",
              " 'heard': 666,\n",
              " 'help': 667,\n",
              " 'himself': 668,\n",
              " 'kept': 669,\n",
              " 'kid': 670,\n",
              " 'muttering': 671,\n",
              " 'pattering': 672,\n",
              " 'ready': 673,\n",
              " 'returning': 674,\n",
              " 'savage': 675,\n",
              " 'sir': 676,\n",
              " 'skurried': 677,\n",
              " 'splendidly': 678,\n",
              " 'timid': 679,\n",
              " 'trotting': 680,\n",
              " 'violently': 681,\n",
              " 'voice': 682,\n",
              " 'age': 683,\n",
              " 'ah': 684,\n",
              " 'changed': 685,\n",
              " 'different': 686,\n",
              " 'everything': 687,\n",
              " 'fanning': 688,\n",
              " 'morning': 689,\n",
              " 'puzzle': 690,\n",
              " 'queer': 691,\n",
              " 'thinking': 692,\n",
              " 'usual': 693,\n",
              " 'yesterday': 694,\n",
              " 'ada': 695,\n",
              " 'besides': 696,\n",
              " 'capital': 697,\n",
              " 'crossed': 698,\n",
              " 'doesn': 699,\n",
              " 'doth': 700,\n",
              " 'five': 701,\n",
              " 'geography': 702,\n",
              " 'goes': 703,\n",
              " 'hair': 704,\n",
              " 'hands': 705,\n",
              " 'hoarse': 706,\n",
              " 'knows': 707,\n",
              " 'lap': 708,\n",
              " 'london': 709,\n",
              " 'mabel': 710,\n",
              " 'mine': 711,\n",
              " 'multiplication': 712,\n",
              " 'paris': 713,\n",
              " 'puzzling': 714,\n",
              " 'repeat': 715,\n",
              " 'ringlets': 716,\n",
              " 'rome': 717,\n",
              " 'seven': 718,\n",
              " 'signify': 719,\n",
              " 'six': 720,\n",
              " 'sorts': 721,\n",
              " 'sounded': 722,\n",
              " 'strange': 723,\n",
              " 'thirteen': 724,\n",
              " 'times': 725,\n",
              " 'try': 726,\n",
              " 'twelve': 727,\n",
              " 'twenty': 728,\n",
              " 'used': 729,\n",
              " 'wrong': 730,\n",
              " 'crocodile': 731,\n",
              " 'his': 732,\n",
              " 'improve': 733,\n",
              " 'nile': 734,\n",
              " 'pour': 735,\n",
              " 'scale': 736,\n",
              " 'shining': 737,\n",
              " 'tail': 738,\n",
              " 'waters': 739,\n",
              " 'cheerfully': 740,\n",
              " 'claws': 741,\n",
              " 'fishes': 742,\n",
              " 'gently': 743,\n",
              " 'grin': 744,\n",
              " 'jaws': 745,\n",
              " 'neatly': 746,\n",
              " 'seems': 747,\n",
              " 'smiling': 748,\n",
              " 'spread': 749,\n",
              " 'welcome': 750,\n",
              " 'alone': 751,\n",
              " 'being': 752,\n",
              " 'burst': 753,\n",
              " 'learn': 754,\n",
              " 'live': 755,\n",
              " 'play': 756,\n",
              " 'poky': 757,\n",
              " 'putting': 758,\n",
              " 'stay': 759,\n",
              " 'sudden': 760,\n",
              " 'till': 761,\n",
              " 'toys': 762,\n",
              " 'avoid': 763,\n",
              " 'cause': 764,\n",
              " 'done': 765,\n",
              " 'guess': 766,\n",
              " 'measure': 767,\n",
              " 'nearly': 768,\n",
              " 'rapidly': 769,\n",
              " 'shrinking': 770,\n",
              " 'bad': 771,\n",
              " 'change': 772,\n",
              " 'declare': 773,\n",
              " 'escape': 774,\n",
              " 'existence': 775,\n",
              " 'frightened': 776,\n",
              " 'narrow': 777,\n",
              " 'speed': 778,\n",
              " 'worse': 779,\n",
              " 'bathing': 780,\n",
              " 'case': 781,\n",
              " 'chin': 782,\n",
              " 'coast': 783,\n",
              " 'conclusion': 784,\n",
              " 'digging': 785,\n",
              " 'general': 786,\n",
              " 'houses': 787,\n",
              " 'lodging': 788,\n",
              " 'machines': 789,\n",
              " 'number': 790,\n",
              " 'railway': 791,\n",
              " 'salt': 792,\n",
              " 'sand': 793,\n",
              " 'sea': 794,\n",
              " 'seaside': 795,\n",
              " 'slipped': 796,\n",
              " 'somehow': 797,\n",
              " 'spades': 798,\n",
              " 'splash': 799,\n",
              " 'station': 800,\n",
              " 'these': 801,\n",
              " 'water': 802,\n",
              " 'wept': 803,\n",
              " 'wherever': 804,\n",
              " 'wooden': 805,\n",
              " 'drowned': 806,\n",
              " 'hadn': 807,\n",
              " 'punished': 808,\n",
              " 'suppose': 809,\n",
              " 'swam': 810,\n",
              " 'hippopotamus': 811,\n",
              " 'nearer': 812,\n",
              " 'something': 813,\n",
              " 'splashing': 814,\n",
              " 'walrus': 815,\n",
              " 'brother': 816,\n",
              " 'grammar': 817,\n",
              " 'harm': 818,\n",
              " 'inquisitively': 819,\n",
              " 'latin': 820,\n",
              " 'speaking': 821,\n",
              " 'swimming': 822,\n",
              " 'talk': 823,\n",
              " 'wink': 824,\n",
              " 'ago': 825,\n",
              " 'animal': 826,\n",
              " 'beg': 827,\n",
              " 'chatte': 828,\n",
              " 'clear': 829,\n",
              " 'conqueror': 830,\n",
              " 'daresay': 831,\n",
              " 'est': 832,\n",
              " 'feelings': 833,\n",
              " 'french': 834,\n",
              " 'fright': 835,\n",
              " 'history': 836,\n",
              " 'leap': 837,\n",
              " 'lesson': 838,\n",
              " 'notion': 839,\n",
              " 'ou': 840,\n",
              " 'pardon': 841,\n",
              " 'quiver': 842,\n",
              " 'sentence': 843,\n",
              " 'understand': 844,\n",
              " 'william': 845,\n",
              " 'passionate': 846,\n",
              " 'shrill': 847,\n",
              " 'angry': 848,\n",
              " 'bristling': 849,\n",
              " 'catching': 850,\n",
              " 'fire': 851,\n",
              " 'lazily': 852,\n",
              " 'licking': 853,\n",
              " 'nicely': 854,\n",
              " 'nurse': 855,\n",
              " 'offended': 856,\n",
              " 'our': 857,\n",
              " 'paws': 858,\n",
              " 'purring': 859,\n",
              " 'quiet': 860,\n",
              " 'show': 861,\n",
              " 'sits': 862,\n",
              " 'soft': 863,\n",
              " 'soothing': 864,\n",
              " 'tone': 865,\n",
              " 'washing': 866,\n",
              " 'we': 867,\n",
              " 'yet': 868,\n",
              " 'always': 869,\n",
              " 'family': 870,\n",
              " 'hated': 871,\n",
              " 'nasty': 872,\n",
              " 'subject': 873,\n",
              " 'trembling': 874,\n",
              " 'vulgar': 875,\n",
              " 'belongs': 876,\n",
              " 'brown': 877,\n",
              " 'commotion': 878,\n",
              " 'conversation': 879,\n",
              " 'curly': 880,\n",
              " 'dinner': 881,\n",
              " 'dog': 882,\n",
              " 'dogs': 883,\n",
              " 'eagerly': 884,\n",
              " 'eyed': 885,\n",
              " 'farmer': 886,\n",
              " 'fetch': 887,\n",
              " 'hundred': 888,\n",
              " 'kills': 889,\n",
              " 'pounds': 890,\n",
              " 'rats': 891,\n",
              " 'says': 892,\n",
              " 'sit': 893,\n",
              " 'sorrowful': 894,\n",
              " 'terrier': 895,\n",
              " 'throw': 896,\n",
              " 'useful': 897,\n",
              " 'called': 898,\n",
              " 'hate': 899,\n",
              " 'pale': 900,\n",
              " 'passion': 901,\n",
              " 'shore': 902,\n",
              " 'softly': 903,\n",
              " 'us': 904,\n",
              " 'animals': 905,\n",
              " 'birds': 906,\n",
              " 'creatures': 907,\n",
              " 'crowded': 908,\n",
              " 'dodo': 909,\n",
              " 'duck': 910,\n",
              " 'eaglet': 911,\n",
              " 'lory': 912,\n",
              " 'party': 913,\n",
              " 'whole': 914,\n",
              " 'caucus': 915,\n",
              " 'iii': 916,\n",
              " 'race': 917,\n",
              " 'tale': 918,\n",
              " 'assembled': 919,\n",
              " 'clinging': 920,\n",
              " 'cross': 921,\n",
              " 'draggled': 922,\n",
              " 'dripping': 923,\n",
              " 'feathers': 924,\n",
              " 'fur': 925,\n",
              " 'looking': 926,\n",
              " 'uncomfortable': 927,\n",
              " 'wet': 928,\n",
              " 'allow': 929,\n",
              " 'argument': 930,\n",
              " 'better': 931,\n",
              " 'consultation': 932,\n",
              " 'course': 933,\n",
              " 'familiarly': 934,\n",
              " 'knowing': 935,\n",
              " 'known': 936,\n",
              " 'last': 937,\n",
              " 'old': 938,\n",
              " 'older': 939,\n",
              " 'positively': 940,\n",
              " 'refused': 941,\n",
              " 'sulky': 942,\n",
              " 'authority': 943,\n",
              " 'cold': 944,\n",
              " 'fixed': 945,\n",
              " 'ring': 946,\n",
              " 'accustomed': 947,\n",
              " 'ahem': 948,\n",
              " 'conquest': 949,\n",
              " 'driest': 950,\n",
              " 'earls': 951,\n",
              " 'edwin': 952,\n",
              " 'favoured': 953,\n",
              " 'important': 954,\n",
              " 'leaders': 955,\n",
              " 'mercia': 956,\n",
              " 'morcar': 957,\n",
              " 'northumbria': 958,\n",
              " 'pope': 959,\n",
              " 'silence': 960,\n",
              " 'submitted': 961,\n",
              " 'usurpation': 962,\n",
              " 'wanted': 963,\n",
              " 'whose': 964,\n",
              " 'shiver': 965,\n",
              " 'ugh': 966,\n",
              " 'frowning': 967,\n",
              " 'politely': 968,\n",
              " 'advisable': 969,\n",
              " 'archbishop': 970,\n",
              " 'canterbury': 971,\n",
              " 'declared': 972,\n",
              " 'him': 973,\n",
              " 'patriotic': 974,\n",
              " 'proceed': 975,\n",
              " 'stigand': 976,\n",
              " 'crossly': 977,\n",
              " 'means': 978,\n",
              " 'replied': 979,\n",
              " 'frog': 980,\n",
              " 'worm': 981,\n",
              " 'atheling': 982,\n",
              " 'conduct': 983,\n",
              " 'continued': 984,\n",
              " 'crown': 985,\n",
              " 'edgar': 986,\n",
              " 'hurriedly': 987,\n",
              " 'insolence': 988,\n",
              " 'meet': 989,\n",
              " 'moderate': 990,\n",
              " 'normans': 991,\n",
              " 'notice': 992,\n",
              " 'offer': 993,\n",
              " 'turning': 994,\n",
              " 'melancholy': 995,\n",
              " 'adjourn': 996,\n",
              " 'adoption': 997,\n",
              " 'energetic': 998,\n",
              " 'immediate': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5irB256GzGJ0"
      },
      "source": [
        "Create the TFIDF matrix\n",
        "======================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsbs2nmkzGJ1"
      },
      "source": [
        "The Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
        "\n",
        "How is TFIDF computed?\n",
        "\n",
        "Tf-Idf is computed by multiplying a local component like term frequency (TF) with a global component, that is, inverse document frequency (IDF) and optionally normalizing the result to unit length.\n",
        "\n",
        "As a result of this, the words that occur frequently across documents will get downweighted.\n",
        "\n",
        "There are multiple variations of formulas for TF and IDF existing. Gensim uses the SMART Information retrieval system that can be used to implement these variations. You can specify what formula to use specifying the smartirs parameter in the TfidfModel. See help(models.TfidfModel) for more details.\n",
        "\n",
        "So, how to get the TFIDF weights?\n",
        "\n",
        "By training the corpus with models.TfidfModel(). Then, apply the corpus within the square brackets of the trained tfidf model. See example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuCYYEmizGJ2",
        "outputId": "39756984-eea0-4f6c-dca4-408aef6aeba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim import models\n",
        "import numpy as np\n",
        "\n",
        "documents = [\"This is the first line\",\n",
        "             \"This is the second sentence\",\n",
        "             \"This third document\"]\n",
        "\n",
        "# Create the Dictionary and Corpus\n",
        "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
        "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n",
        "\n",
        "# Show the Word Weights in Corpus\n",
        "for doc in corpus:\n",
        "    print([[mydict[id], freq] for id, freq in doc])\n",
        "\n",
        "print('======TF-IDF======')\n",
        "# Create the TF-IDF model\n",
        "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
        "\n",
        "# Show the TF-IDF weights\n",
        "for doc in tfidf[corpus]:\n",
        "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['first', 1], ['is', 1], ['line', 1], ['the', 1], ['this', 1]]\n",
            "[['is', 1], ['the', 1], ['this', 1], ['second', 1], ['sentence', 1]]\n",
            "[['this', 1], ['document', 1], ['third', 1]]\n",
            "======TF-IDF======\n",
            "[['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
            "[['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
            "[['document', 0.71], ['third', 0.71]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbPyRMsEzGJ8"
      },
      "source": [
        "Notice the difference in weights of the words between the original corpus and the tfidf weighted corpus.\n",
        "\n",
        "The words ‘is’ and ‘the’ occur in two documents and were weighted down. The word ‘this’ appearing in all three documents was removed altogether. In simple terms, words that occur more frequently across the documents get smaller weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNOwBxDzGKB"
      },
      "source": [
        "Gensim provides an inbuilt API to download popular text datasets and word embedding models.\n",
        "\n",
        "A comprehensive list of available datasets and models is maintained here: https://raw.githubusercontent.com/RaRe-Technologies/gensim-data/master/list.json.\n",
        "\n",
        "Using the API to download the dataset is as simple as calling the api.load() method with the right data or model name.\n",
        "\n",
        "Now you know how to download datasets and pre-trained models with gensim.\n",
        "\n",
        "Let’s download the text8 dataset, which is nothing but the “First 100,000,000 bytes of plain text from Wikipedia”. Then, from this, we will generate our word2vec model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfJgwO8zGKQ"
      },
      "source": [
        "Train Word2Vec model using gensim\n",
        "================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8In_GE9zGKQ"
      },
      "source": [
        "A word embedding model is a model that can provide numerical vectors for a given word. Using the Gensim’s downloader API, you can download pre-built word embedding models like word2vec, fasttext, GloVe and ConceptNet. These are built on large corpuses of commonly occurring text data such as wikipedia, google news etc.\n",
        "\n",
        "However, if you are working in a specialized niche such as technical documents, you may not able to get word embeddings for all the words. So, in such cases its desirable to train your own model.\n",
        "\n",
        "Gensim’s Word2Vec implementation let’s you train your own word embedding model for a given corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNcWU-MazGKR",
        "outputId": "59276977-5581-4508-d12e-a8b58ce32cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download dataset\n",
        "dataset = api.load(\"text8\")\n",
        "data = [d for d in dataset]\n",
        "\n",
        "# Split the data into 2 parts.\n",
        "data_part1 = data[:1000]\n",
        "\n",
        "# Train Word2Vec model. Defaults result vector size = 100\n",
        "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())\n",
        "\n",
        "# Get the word vector for given word\n",
        "model.wv['topic']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.20916158,  0.7087789 , -0.5962847 ,  0.6638988 ,  0.76197356,\n",
              "        0.0174288 , -0.6005232 ,  0.03088057, -0.43005309,  0.43407992,\n",
              "        0.51272565,  0.680744  , -0.38010162, -1.145793  , -0.33229455,\n",
              "        0.5037972 ,  1.0488638 , -0.50938714,  0.10804344, -0.65920526,\n",
              "        0.6726281 ,  0.49863836,  0.77911323,  0.02854261, -0.08719321,\n",
              "       -0.36370376,  0.56738514, -1.3646485 , -0.03270853,  0.5631858 ,\n",
              "       -0.92326534, -0.28342935, -0.53743184, -0.1441928 , -0.40874404,\n",
              "        0.0752447 , -0.05068542,  0.38175386,  0.04880134, -0.36099538,\n",
              "       -0.7279049 ,  0.7583246 , -0.41334948,  1.2167485 , -0.5144419 ,\n",
              "       -0.03869675,  0.42988262, -0.01132099, -0.09705407, -0.20196539,\n",
              "        1.150143  ,  0.32986695,  0.4846633 , -0.20631927,  0.8978153 ,\n",
              "        0.48524848,  0.25955907,  0.6586873 ,  0.22822939, -0.14310484,\n",
              "       -1.1186346 ,  0.46435088, -1.2563497 ,  0.08751276, -0.9244074 ,\n",
              "        0.6224192 , -0.21488847,  0.0789334 ,  0.49726826, -0.4790602 ,\n",
              "       -0.6230556 , -0.5921655 ,  0.5423055 ,  0.55064625,  0.34066385,\n",
              "        0.12012593, -0.9634433 , -1.1110283 ,  1.1716172 , -1.2579246 ,\n",
              "       -0.82811284, -0.14270289, -0.35983118,  0.21583956, -0.53253144,\n",
              "        0.5868654 , -0.37101346, -1.0296174 ,  0.42222407, -0.77480924,\n",
              "       -1.8521007 ,  1.9035258 , -0.0732134 ,  1.4480747 , -0.5731205 ,\n",
              "       -0.8435016 , -1.2925063 , -0.34382954,  0.68060446,  0.6640289 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cD-nQkezGKV",
        "outputId": "393c33b6-4a26-4d57-970d-32ce25a1cf0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#get similar words\n",
        "model.most_similar('topic')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b100799fba9b>:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  model.most_similar('topic')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('interpretation', 0.746008038520813),\n",
              " ('discussion', 0.7281632423400879),\n",
              " ('discourse', 0.7196004390716553),\n",
              " ('consensus', 0.7183263301849365),\n",
              " ('facts', 0.7101843357086182),\n",
              " ('opinions', 0.7097778916358948),\n",
              " ('explanation', 0.6956576704978943),\n",
              " ('viewpoint', 0.691872239112854),\n",
              " ('discussions', 0.6886538863182068),\n",
              " ('debate', 0.6881909370422363)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayC5x1l_zGKZ"
      },
      "source": [
        "# Save and Load Model\n",
        "model.save('newmodel')\n",
        "model = Word2Vec.load('newmodel')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl4WuZChzGKw",
        "outputId": "2d4b7780-ed31-43fa-8f99-91f56ddff23e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the word vector for given word\n",
        "model.wv['cat']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.10987332,  0.2036283 ,  0.16947654,  0.721487  ,  0.58792967,\n",
              "       -1.0317556 , -0.6475652 , -0.6802951 , -0.21127282, -1.2004807 ,\n",
              "       -0.91009104, -0.14582942, -0.06389111, -0.25357106, -1.0658027 ,\n",
              "        0.09959983,  0.60143715,  0.69439656, -0.60844666,  0.2759683 ,\n",
              "        0.3133773 ,  1.1956104 , -0.5277805 , -0.33228645,  0.29380295,\n",
              "       -0.17669582, -0.09216116, -0.44687486,  0.9891061 ,  0.49398538,\n",
              "        0.27257514, -0.12295306,  1.1147054 ,  1.4342041 ,  0.81141806,\n",
              "       -0.11075892, -0.37892073,  0.77847743, -0.94997275, -0.5614052 ,\n",
              "        0.7779288 , -0.6108251 ,  0.8434496 ,  1.0098668 , -1.3086749 ,\n",
              "       -0.02915825, -1.3802407 , -0.40359256, -0.7648273 , -0.14008261,\n",
              "       -0.23964597,  0.21774319, -0.49330592, -0.9623035 ,  0.22438283,\n",
              "        0.01699814,  0.5388627 , -0.6713309 , -0.06798438,  0.23533534,\n",
              "       -0.342082  , -0.3905778 , -1.7710574 , -1.1909999 , -0.7278035 ,\n",
              "        1.2562726 , -2.2671757 ,  2.1929944 , -0.45203242,  0.55238545,\n",
              "        0.59037966, -0.39405906,  0.2989988 ,  0.76770335, -0.32434508,\n",
              "       -0.40560415,  1.0229547 , -1.5765072 ,  0.07274324,  1.2369719 ,\n",
              "       -0.19424492,  1.3328509 ,  0.38695917, -0.5346802 ,  0.43618688,\n",
              "       -0.9992664 , -0.61302173, -0.72093505, -0.81088316,  0.23728758,\n",
              "       -0.75297827, -0.4932478 ,  2.4695642 ,  1.7513204 , -0.09955182,\n",
              "       -0.88431793, -0.5398629 , -0.4676346 , -0.72398144, -1.1641003 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoaEF_dzGK0",
        "outputId": "22033ec2-c3af-4307-ec03-1327a8bf4413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#get similar words\n",
        "model.wv.most_similar('cat')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dog', 0.8086656332015991),\n",
              " ('bee', 0.7739020586013794),\n",
              " ('sweet', 0.7658553719520569),\n",
              " ('bird', 0.760177731513977),\n",
              " ('flower', 0.7451235055923462),\n",
              " ('goat', 0.736929714679718),\n",
              " ('frog', 0.7342759966850281),\n",
              " ('aloe', 0.7337996959686279),\n",
              " ('cow', 0.7215234041213989),\n",
              " ('bean', 0.7164335250854492)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAJb-6sizGK3",
        "outputId": "c7bd22d7-2186-4915-a07a-a987ebc50642",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#get similarity between two words\n",
        "model.wv.similarity('dog','cat')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80866563"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh-8EvnnzGK7"
      },
      "source": [
        "Import pre-trainined word2vec\n",
        "============================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOmrWPsVzGK9"
      },
      "source": [
        "We just saw how to get the word vectors for Word2Vec model we just trained. However, gensim lets you download state of the art pretrained models through the downloader API. Let’s see how to extract the word vectors from a couple of these models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSYjX31vzGK-",
        "outputId": "9a94a524-9b3d-4c72-ee6c-1ad2309b359c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download the models (1660MB)\n",
        "word2vec_model300 = api.load('word2vec-google-news-300')\n",
        "\n",
        "#get similar words\n",
        "word2vec_model300.wv.most_similar('support')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-7760d1af93ef>:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  word2vec_model300.wv.most_similar('support')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supporting', 0.6251285076141357),\n",
              " ('suport', 0.6071150302886963),\n",
              " ('suppport', 0.6053199768066406),\n",
              " ('Support', 0.6044273376464844),\n",
              " ('supported', 0.6009396910667419),\n",
              " ('backing', 0.6007589101791382),\n",
              " ('supports', 0.5269277095794678),\n",
              " ('assistance', 0.5207138061523438),\n",
              " ('sup_port', 0.5192490220069885),\n",
              " ('supportive', 0.5110025405883789)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKWnzRfzGLB",
        "outputId": "786f91ea-4318-4ba6-833d-d729f52572e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#download a model based on Glove (128MB)\n",
        "glove_model100 = api.load('glove-wiki-gigaword-100')\n",
        "#get similar words\n",
        "glove_model100.wv.most_similar('dog')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-7766f0496f8d>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  glove_model100.wv.most_similar('dog')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.8798074722290039),\n",
              " ('dogs', 0.8344309329986572),\n",
              " ('pet', 0.7449564933776855),\n",
              " ('puppy', 0.723637580871582),\n",
              " ('horse', 0.7109653353691101),\n",
              " ('animal', 0.6817063093185425),\n",
              " ('pig', 0.6554172039031982),\n",
              " ('boy', 0.6545307636260986),\n",
              " ('cats', 0.6471933126449585),\n",
              " ('rabbit', 0.6468630433082581)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_M1OKhBaL4X",
        "outputId": "b5c2c235-4e08-4479-b2a9-b15688fe9d88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glove_model100.wv.similarity('dog','cat')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c299cfb2c74c>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  glove_model100.wv.similarity('dog','cat')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8798075"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}